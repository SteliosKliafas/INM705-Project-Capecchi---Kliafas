{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from dataset import FaceInTheWild\n",
    "from model import Discriminator, Generator, DCGAN\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torchvision\n",
    "import subprocess\n",
    "from pytorch_fid import fid_score as compute_fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "# define the device to use\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "save_dir = os.path.join(os.curdir, 'saved_models')\n",
    "checkpoint_path = os.path.join(save_dir, 'checkpoint.pth')\n",
    "trained_model_path = os.path.join(save_dir, 'trained_model.pth')\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "\n",
    "##### PROCEDURE TO GET THE IMAGES #####\n",
    "\n",
    "NAMES_DIR_PATH = os.path.join(os.curdir, 'dataset/lfw')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_people_names():\n",
    "    \"\"\"\n",
    "    This function returns a list of the directories name of each person in the dataset.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return os.listdir(NAMES_DIR_PATH)\n",
    "\n",
    "def get_images_label(people):\n",
    "    \"\"\"\n",
    "    This function iterates through an array of name's people to access the\n",
    "    directory that contains the corresponding images and creates a new directory\n",
    "    containing all the pictures of the people\n",
    "    :param people: the list of the people names in the dataset.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    destination_path = os.path.join(os.curdir, 'images')\n",
    "    if not os.path.exists(destination_path):\n",
    "        os.mkdir('images')\n",
    "    if len(os.listdir(destination_path)) >=2:\n",
    "        return os.listdir(destination_path)\n",
    "    for person in people:\n",
    "        path = os.path.join(NAMES_DIR_PATH, person)\n",
    "        images = os.listdir(path)\n",
    "        for image in images:\n",
    "            image_path = os.path.join(path, image)\n",
    "            if not os.path.exists(os.path.join(destination_path, image)):\n",
    "                shutil.copy(image_path, destination_path)\n",
    "    print('Images parsed correctly.')\n",
    "    print('Total number of images: {}'.format(len(os.listdir(destination_path))))\n",
    "    images = os.listdir(destination_path)\n",
    "    return images\n",
    "\n",
    "\n",
    "def visualize_sample_image(n_images):\n",
    "    \"\"\"\n",
    "    This function visualize n_images, sampled randomly\n",
    "    from the dataset\n",
    "    :param n_images: number of image to print per axis\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for idx, image in enumerate(train_dataloader):\n",
    "        if idx + 1 > n_images**2:\n",
    "            break\n",
    "        plt.subplot(n_images, n_images, idx + 1)\n",
    "        image = image[0].permute(1, 2, 0)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def split_test_train_images(data, source_path, destination_path, phase):\n",
    "    if phase == 'train':\n",
    "        start = 0\n",
    "        limit = int(len(data)*0.8)\n",
    "        if os.listdir(destination_path):\n",
    "            return os.listdir(destination_path)\n",
    "    elif phase == 'test':\n",
    "        start = int(len(data)*0.8)\n",
    "        limit = len(data)\n",
    "        if os.listdir(destination_path):\n",
    "            return os.listdir(destination_path)\n",
    "\n",
    "    for img in data[start:limit]:\n",
    "        img_path = os.path.join(source_path, img)\n",
    "        shutil.move(img_path, destination_path)\n",
    "\n",
    "    return os.listdir(destination_path)\n",
    "\n",
    "def format_time(start, end):\n",
    "    \"\"\"\n",
    "    Computes the interval time between a start and an end point.\n",
    "    :param start: starting time\n",
    "    :param end: ending time\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    elapsed_time = end - start\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_secs, elapsed_mins\n",
    "\n",
    "def load_from_checkpoint():\n",
    "    epoch_idx = 0\n",
    "    d_loss_history = []\n",
    "    g_loss_history = []\n",
    "    d_x_history = []\n",
    "    d_g_z_history = []\n",
    "    fid_score_history = []\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        loaded_checkpoint = torch.load(checkpoint_path, map_location=torch.device(device))\n",
    "        print('Checkpoint found. Restore from [{}/{}] epoch.'.format(loaded_checkpoint['epoch'], n_epochs))\n",
    "        epoch_idx = loaded_checkpoint['epoch']\n",
    "        dcgan.load_state_dict(loaded_checkpoint['dcgan_model'])\n",
    "        dcgan.discriminator.optim.load_state_dict(loaded_checkpoint['d_optim'])\n",
    "        dcgan.generator.optim.load_state_dict(loaded_checkpoint['g_optim'])\n",
    "        d_loss_history = loaded_checkpoint['d_loss_history']\n",
    "        g_loss_history = loaded_checkpoint['g_loss_history']\n",
    "        d_x_history = loaded_checkpoint['d_x_history']\n",
    "        d_g_z_history = loaded_checkpoint['d_g_z_history']\n",
    "        fid_score_history = loaded_checkpoint['fid_score_history']\n",
    "        return epoch_idx, d_loss_history, g_loss_history, d_x_history, d_g_z_history, fid_score_history\n",
    "    else:\n",
    "        return epoch_idx, d_loss_history, g_loss_history, d_x_history, d_g_z_history, fid_score_history\n",
    "\n",
    "def save_trained_model(state_dict):\n",
    "    print('Saving trained model...')\n",
    "    torch.save(state_dict, trained_model_path)\n",
    "    print('Model saved correctly.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images parsed correctly.\n",
      "Total number of images: 13233\n",
      "...Initialize weights from N(0, 0.02)...\n",
      "Weights initialized.\n",
      "...Initialize weights from N(0, 0.02)...\n",
      "Weights initialized.\n"
     ]
    }
   ],
   "source": [
    "# get the names of the people in the dataset.\n",
    "people_names = get_people_names()\n",
    "# use those names to access each directory and retrieve the images.\n",
    "# since we are implementing a GAN model, we don't care about the people's name\n",
    "# we only want to retrieve the image.\n",
    "data = get_images_label(people_names)\n",
    "# create two folder to separate train images from test images\n",
    "images_path = os.path.join(os.curdir, 'images')\n",
    "train_path = os.path.join(images_path, 'train')\n",
    "test_path = os.path.join(images_path, 'test')\n",
    "if not os.path.exists(train_path):\n",
    "    os.mkdir(train_path)\n",
    "if not os.path.exists(test_path):\n",
    "    os.mkdir(test_path)\n",
    "\n",
    "train_images = split_test_train_images(data, images_path, train_path, phase='train')\n",
    "test_images = split_test_train_images(data, images_path, test_path, phase='test')\n",
    "# create the dataset\n",
    "train_data = FaceInTheWild(train_images, 'train')\n",
    "test_data = FaceInTheWild(test_images, 'test')\n",
    "\n",
    "## building the model\n",
    "latent_vector = torch.randn((100, 1))\n",
    "input_size = latent_vector.shape[0]\n",
    "generator = Generator(input_size).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "dcgan = DCGAN(generator, discriminator, input_size).to(device)\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64\n",
    "optim_params = {'lr': 0.0002, 'betas': (0.5, 0.999)}\n",
    "n_epochs = 10\n",
    "# dataloaders\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "\n",
    "real_img = 1\n",
    "fake_img = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \"\"\"\n",
    "    Compute the FID score. We use the open-source library pytorch-fid\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dcgan.discriminator.eval()\n",
    "    dcgan.generator.eval()\n",
    "    generated_path = os.path.join(images_path, 'generated')\n",
    "    test_path = os.path.join(images_path, 'test')\n",
    "    if os.path.exists(generated_path):\n",
    "        shutil.rmtree(generated_path)\n",
    "    os.mkdir(generated_path)\n",
    "    # feedforward pass all the test images through the generator\n",
    "    for idx, _ in enumerate(test_dataloader):\n",
    "        noise = torch.randn((1, input_size, 1, 1), dtype=torch.float).to(device)\n",
    "        gen_img = dcgan.generator(noise)\n",
    "        gen_img = gen_img.reshape(3, 64, 64)\n",
    "        gen_img = (gen_img * 0.5) + 0.5\n",
    "        gen_img = torchvision.transforms.ToPILImage(mode='RGB')(gen_img)\n",
    "        gen_img.save(generated_path+'/gen_'+str(idx)+'.jpg')\n",
    "    fid_score = compute_fid.calculate_fid_given_paths([generated_path, test_path], 50, device, 2048)\n",
    "    return fid_score\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "n_epochs = 2000\n",
    "print_every = 100\n",
    "def train_loop():\n",
    "    epoch_idx, d_loss_history, g_loss_history, d_x_history, d_g_z_history, fid_score_history = load_from_checkpoint()\n",
    "    for epoch in range(epoch_idx, n_epochs):\n",
    "        d_loss_running = []\n",
    "        g_loss_running = []\n",
    "        d_x_running = []\n",
    "        d_g_z_running = []\n",
    "        start_time = time.time()\n",
    "        for idx, batch in enumerate(train_dataloader):\n",
    "            # TRAIN PROCEDURE\n",
    "            # get batch of real images\n",
    "            # each image has a shape of [NxCxHxW] --> [64x3x64x64]\n",
    "            d_loss, g_loss, D_X, D_G_z = dcgan(batch)\n",
    "            # store losses\n",
    "            d_loss_running.append(d_loss)\n",
    "            g_loss_running.append(g_loss)\n",
    "            d_x_running.append(D_X)\n",
    "            d_g_z_running.append(D_G_z)\n",
    "\n",
    "        d_loss_history.append(np.mean(d_loss_running).item())\n",
    "        g_loss_history.append(np.mean(g_loss_running).item())\n",
    "        d_x_history.append(np.mean(d_x_running).item())\n",
    "        d_g_z_history.append(np.mean(d_g_z_running).item())\n",
    "        if epoch % print_every == 0:\n",
    "            # visualize a sample of generated images once every n epoch\n",
    "            fid_score = evaluate()\n",
    "            fid_score_history.append(fid_score)\n",
    "            gen_images = dcgan.apply_knowledge()\n",
    "            dcgan.visualize_sample(gen_images)\n",
    "            checkpoint = {'epoch': epoch,\n",
    "                        'dcgan_model': dcgan.state_dict(),\n",
    "                        'd_optim': dcgan.discriminator.optim.state_dict(),\n",
    "                        'g_optim': dcgan.generator.optim.state_dict(),\n",
    "                        'd_loss_history': d_loss_history,\n",
    "                        'g_loss_history': g_loss_history,\n",
    "                        'd_x_history': d_x_history,\n",
    "                        'd_g_z_history': d_g_z_history,\n",
    "                        'fid_score_history': fid_score_history}\n",
    "            print('Checkpoint [%.d/%.d], D_Loss: %.4f, G_Loss: %.4f, D(x): %.4f, D(G(z)): %.4f, FID: %.4f' % (epoch,\n",
    "                                                                                                     n_epochs,\n",
    "                                                                                                     d_loss_running[-1],\n",
    "                                                                                                     g_loss_running[-1],\n",
    "                                                                                                     d_x_history[-1],\n",
    "                                                                                                     d_g_z_history[-1],\n",
    "                                                                                                     fid_score_history[-1]),\n",
    "              end='| ')\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "        end_time = time.time()\n",
    "        elapsed_sec, elapsed_min = format_time(start_time, end_time)\n",
    "        print('Epoch [{}]/[{}], eta: {}m {}s'.format(epoch, n_epochs, elapsed_min, elapsed_sec))\n",
    "    print('Training Complete.')\n",
    "    save_trained_model(dcgan.state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [0/187], D_Loss: 1.3864765167236328, G_Loss: 1.3864765167236328, D(x): 0.49982768297195435, D(G(z)): 0.4889525771141052\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-6590b82f66d8>\", line 1, in <module>\n",
      "    train_loop()\n",
      "  File \"<ipython-input-5-0a2c7bd90056>\", line 11, in train_loop\n",
      "    d_loss, g_loss, D_X, D_G_z = dcgan(batch)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/Users/tommasocapecchi/City/705/Coursework/model.py\", line 42, in forward\n",
      "    d_loss, gen_imgs, D_x = self.train_discriminator(input)\n",
      "  File \"/Users/tommasocapecchi/City/705/Coursework/model.py\", line 76, in train_discriminator\n",
      "    d_loss.backward()\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\", line 245, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 145, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/anaconda3/lib/python3.8/inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/anaconda3/lib/python3.8/inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/anaconda3/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/anaconda3/lib/python3.8/inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/opt/anaconda3/lib/python3.8/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/opt/anaconda3/lib/python3.8/posixpath.py\", line 424, in _joinrealpath\n",
      "    newpath = join(path, name)\n",
      "  File \"/opt/anaconda3/lib/python3.8/posixpath.py\", line 82, in join\n",
      "    for b in map(os.fspath, p):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[0;32m<ipython-input-6-6590b82f66d8>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrain_loop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-5-0a2c7bd90056>\u001B[0m in \u001B[0;36mtrain_loop\u001B[0;34m()\u001B[0m\n\u001B[1;32m     10\u001B[0m             \u001B[0;31m# each image has a shape of [NxCxHxW] --> [64x3x64x64]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m             \u001B[0md_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mg_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mD_X\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mD_G_z\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdcgan\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m             \u001B[0;31m# store losses\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n",
      "\u001B[0;32m~/City/705/Coursework/model.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0;31m# Train Discriminator --> Maximize: log(D(x)) + log(1-(D(G(z)))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m         \u001B[0md_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgen_imgs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mD_x\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_discriminator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     43\u001B[0m         \u001B[0;31m# train Generator --> Maximize: log(D(G(z)))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/City/705/Coursework/model.py\u001B[0m in \u001B[0;36mtrain_discriminator\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     75\u001B[0m         \u001B[0;31m# compute gradients\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 76\u001B[0;31m         \u001B[0md_loss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     77\u001B[0m         \u001B[0;31m# update weights\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    244\u001B[0m                 inputs=inputs)\n\u001B[0;32m--> 245\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    246\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 145\u001B[0;31m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[1;32m    146\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2044\u001B[0m                         \u001B[0;31m# in the engines. This should return a list of strings.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2045\u001B[0;31m                         \u001B[0mstb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_render_traceback_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2046\u001B[0m                     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2045\u001B[0m                         \u001B[0mstb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_render_traceback_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2046\u001B[0m                     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2047\u001B[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001B[0m\u001B[1;32m   2048\u001B[0m                                             value, tb, tb_offset=tb_offset)\n\u001B[1;32m   2049\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1434\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1435\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1436\u001B[0;31m         return FormattedTB.structured_traceback(\n\u001B[0m\u001B[1;32m   1437\u001B[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001B[1;32m   1438\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1334\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmode\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mverbose_modes\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1335\u001B[0m             \u001B[0;31m# Verbose modes need a full traceback\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1336\u001B[0;31m             return VerboseTB.structured_traceback(\n\u001B[0m\u001B[1;32m   1337\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0metype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtb_offset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnumber_of_lines_of_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1338\u001B[0m             )\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1191\u001B[0m         \u001B[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1192\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1193\u001B[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001B[0m\u001B[1;32m   1194\u001B[0m                                                                tb_offset)\n\u001B[1;32m   1195\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mformat_exception_as_a_whole\u001B[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001B[0m\n\u001B[1;32m   1149\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1150\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1151\u001B[0;31m         \u001B[0mlast_unique\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfind_recursion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0morig_etype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1152\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1153\u001B[0m         \u001B[0mframes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat_records\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast_unique\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mfind_recursion\u001B[0;34m(etype, value, records)\u001B[0m\n\u001B[1;32m    449\u001B[0m     \u001B[0;31m# first frame (from in to out) that looks different.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    450\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_recursion_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0metype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 451\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    452\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    453\u001B[0m     \u001B[0;31m# Select filename, lineno, func_name to track frames with\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "train_loop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, d_loss_history, g_loss_history, d_x_history, d_g_z_history, fid_score_history = load_from_checkpoint()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_g_vs_d(d_history, g_history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title('Generator Loss vs Discriminator Loss')\n",
    "    plt.plot(d_history, label='Generator')\n",
    "    plt.plot(g_history, label='Discriminator')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_fid(fid_history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title('FID score History')\n",
    "    plt.plot(fid_history, label='FID-Score')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_g_vs_d(d_loss_history, g_loss_history)\n",
    "plot_g_vs_d(d_x_history, d_g_z_history)\n",
    "plot_fid(fid_score_history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}