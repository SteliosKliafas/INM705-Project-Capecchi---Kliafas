import torch.nn as nn
import torch
from torch.optim import Adam, SGD
import matplotlib.pyplot as plt

device = torch.device('cpu')
if torch.cuda.is_available():
    device = torch.device('cuda')
print(device)


class DCGAN(nn.Module):

    def __init__(self, generator, discriminator, input_size):
        super(DCGAN, self).__init__()
        self.generator = generator
        self.discriminator = discriminator
        self.input_size = input_size
        self.criterion = nn.BCEWithLogitsLoss()
        # batch of random noise used to visualize how good the
        self.sample_noise = torch.randn((64, self.input_size, 1, 1), dtype=torch.float).to(device)

    def forward(self, input):
        """
        This function implements the main procedure
        to train the DCGAN. It is mainly composed by two
        practical steps which will improve the convergence
        during training, as suggested by https://github.com/soumith/ganhacks.
        1- maximize the quantity log(D(x)) + log(1-(D(G(z))) for the Discriminator
        2- maximize the quantity log(D(G(z))) for the Generator.
        :param input: batch of images sampled from the dataset
        :return d_loss, g_loss: the losses generated by the discriminator and the generator respectively.
        """
        input = input.to(device)
        batch_size = input.shape[0]
        # Because we are using the BCE, by alternating
        # labels of zeros and ones, we can easily compute
        # the two terms of the first equation and then sum those
        # values, thus maximizing the loss.

        # Train Discriminator --> Maximize: log(D(x)) + log(1-(D(G(z)))
        d_loss, gen_imgs, D_x = self.train_discriminator(input)
        # train Generator --> Maximize: log(D(G(z)))
        g_loss, D_G_z = self.train_generator(gen_imgs)
        return d_loss.item(), g_loss.item(), D_x, D_G_z

    def train_discriminator(self, input):
        self.discriminator.train()
        self.discriminator.optim.zero_grad()

        batch_size = input.shape[0]

        fake_z_vectors = torch.randn(batch_size, self.input_size, 1, 1).to(device)

        # get loss for the real images --> log(D(x))
        pred_real = self.discriminator(input).reshape(-1)  # -> D(x)
        # average D(x) over the batch size
        D_x = torch.sigmoid(pred_real.mean()).item()
        # create vector of 1s for the real images. Must be of the same shape as pred_real
        # real_labels = (torch.ones(pred_real.shape, dtype=torch.float)*0.9).to(device)
        real_labels = (torch.randint(low=7, high=13, size=pred_real.shape)*0.1).to(device)
        real_img_loss = self.criterion(pred_real, real_labels)

        # get loss for the fake images --> log(1-(D(G(z)))

        # generate fake images from batch of latent vectors
        gen_imgs = self.generator(fake_z_vectors)  # -> G(z)
        # forward pass through the discriminator to get predictions
        pred_gen = self.discriminator(gen_imgs.detach()).reshape(-1)  # -> D(G(z))
        # create vector of 0s for the generated images. Must be of the same shape as pred_gen
        # fake_label = (torch.ones(pred_gen.shape, dtype=torch.float)*0.1).to(device)
        fake_label = (torch.randint(low=0, high=4, size=pred_gen.shape, dtype=torch.float)*0.1).to(device)
        gen_img_loss = self.criterion(pred_gen, fake_label)  # -> log(1-D(G(z)))

        # combine the two losses
        d_loss = real_img_loss + gen_img_loss
        # compute gradients
        d_loss.backward()
        # update weights
        self.discriminator.optim.step()

        return d_loss, gen_imgs, D_x

    def train_generator(self, gen_imgs):
        self.generator.train()
        self.generator.optim.zero_grad()

        # classify previous Generator's output
        pred_gen = self.discriminator(gen_imgs).reshape(-1)  # -> D(G(z))
        # average D(G(z)) over the batch size
        D_G_z = torch.sigmoid(pred_gen.mean()).item()
        # use the real labels to compute the loss
        real_labels = torch.ones(pred_gen.shape, dtype=torch.float).to(device)
        # we use the real labels because we want to maximize the quantity log(D(G(z)))
        # instead of minimizing the quantity log(1-(D(G(z)))
        # because we use the BCEloss, this is achieved by considering the quantity log(x)
        # instead of the quantity log(1-x)
        g_loss = self.criterion(pred_gen, real_labels)  # log(D(G(z)))
        # compute gradients
        g_loss.backward()
        # update weights
        self.generator.optim.step()

        return g_loss, D_G_z

    def apply_knowledge(self):
        with torch.no_grad():
            gen_images = self.generator(self.sample_noise)
        # list which contains a series of images that are
        # progressively generated during the training phase
        return gen_images

    def visualize_sample(self, gen_images):
        plt.figure(figsize=(8, 8))
        # we assume to have batches of size to the power of two, e.g 64, 128, 256...
        n_img_per_axis = int(gen_images.shape[0] / 8)
        for idx, img in enumerate(gen_images):
            if idx + 1 > n_img_per_axis ** 2:
                break
            plt.subplot(n_img_per_axis, n_img_per_axis, idx + 1)
            img = (img * 0.5) + 0.5
            img = img.permute(1, 2, 0)
            plt.imshow(img.cpu())
            plt.axis('off')
        plt.show()
        plt.close()


class Discriminator(nn.Module):
    """
    This class implements the Discriminator of the DCGAN. We followed the
    guidelines expressed in the following paper of Alec Radford, Luke Metz and Soumith Chintala
    https://arxiv.org/pdf/1511.06434.pdf.
    """

    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            # we work with batches -> [N x n_channels x 64 x 64]
            # output dimensions at each layer we apply the formula [out = ((i+2p-k)/s)+1]
            # Convolution is used to DOWNSAMPLE the image

            # output [N x 64 x 32 x 32]
            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # output [N x 128 x 16 x 16]
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            # output [N x 256 x 8 x 8]
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            # output [N x 512 x 4 x 4]
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            # output [N x 1 x 1 x 1]
            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),
            # nn.Sigmoid()
        ).to(device)

        self.optim = Adam(self.model.parameters(), lr=0.0002, betas=(0.5, 0.999))
        self.init_weights()

    def forward(self, x):
        output = self.model(x)
        return output

    def init_weights(self):
        print('...Initialize weights from N(0, 0.02)...')
        for layer in self.model:
            layer_type = type(layer)
            if layer_type == nn.Conv2d or layer_type == nn.BatchNorm2d:
                nn.init.normal_(layer.weight, 0, 0.02)
        print('Weights initialized.')


class Generator(nn.Module):
    """
    This class implements the Generator of the DCGAN following the guidelines
    detailed expressed in the following paper of Alec Radford, Luke Metz and Soumith Chintala
    https://arxiv.org/pdf/1511.06434.pdf.
    """

    def __init__(self, z_size):
        super(Generator, self).__init__()
        self.z_size = z_size
        # the sequence of [Conv Transpose-BatchNorm-Act Function] used to generate
        # a volume of 3x64x64 (an image) from the latent vector. To compute the correct
        # output dimensions at each layer we apply the formula [out = s(i-1)+k-2p]
        # Transpose Convolution is used to UPSAMPLE the image
        self.model = nn.Sequential(
            # input [N x noise_channel x 1 x 1
            # output [N x 512 x 4 x 4]
            nn.ConvTranspose2d(self.z_size, 512, kernel_size=4, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(512),
            nn.Dropout(0.1),
            nn.ReLU(inplace=True),
            # output [N x 256 x 8 x 8]
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            # output [N x 128 x 16 x 16]
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.Dropout(0.1),
            nn.ReLU(inplace=True),
            # output [N x 64 x 32 x 32]
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            # output [N x 3 x 64 x 64]
            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),
            nn.Tanh()
        ).to(device)

        self.optim = Adam(self.model.parameters(), lr=0.0002, betas=(0.5, 0.999))
        self.init_weights()

    def forward(self, z):
        output = self.model(z)
        return output

    def init_weights(self):
        print('...Initialize weights from N(0, 0.02)...')
        for layer in self.model:
            layer_type = type(layer)
            if layer_type == nn.ConvTranspose2d or layer_type == nn.BatchNorm2d:
                nn.init.normal_(layer.weight, 0, 0.02)
        print('Weights initialized.')
